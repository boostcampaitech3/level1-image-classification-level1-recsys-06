{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image classification of mask wearing state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 파일은 boostcamp AI Tech에서 제공하는 sample_submission.ipynb과 이민준 캠퍼님의 resnet18 base code를 참고하여 작성헸습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Libarary 불러오기 및 경로설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "import albumentations as A\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms, utils\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize, RandomHorizontalFlip\n",
    "from baseline.dataset import AddGaussianNoise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/opt/ml/input/data/train/images'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 훈련 데이터셋 폴더 경로 지정\n",
    "train_dir = '/opt/ml/input/data/train'\n",
    "train_img_dir_path = os.path.join(train_dir, 'images')\n",
    "train_img_dir_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터셋 폴더 경로를 지정해주세요.\n",
    "test_dir = '/opt/ml/input/data/eval'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>race</th>\n",
       "      <th>age</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000001</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>45</td>\n",
       "      <td>000001_female_Asian_45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000002</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>52</td>\n",
       "      <td>000002_female_Asian_52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000004</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>54</td>\n",
       "      <td>000004_male_Asian_54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000005</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>58</td>\n",
       "      <td>000005_female_Asian_58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000006</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>59</td>\n",
       "      <td>000006_female_Asian_59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  gender   race  age                    path\n",
       "0  000001  female  Asian   45  000001_female_Asian_45\n",
       "1  000002  female  Asian   52  000002_female_Asian_52\n",
       "2  000004    male  Asian   54    000004_male_Asian_54\n",
       "3  000005  female  Asian   58  000005_female_Asian_58\n",
       "4  000006  female  Asian   59  000006_female_Asian_59"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('./input/data/train/train.csv')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Dataset 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir_name 디렉토리 하부에 있는 모든 파일을 검색합니다. \n",
    "def search(dir_name, result):\n",
    "    try:\n",
    "        filenames = os.listdir(dir_name)\n",
    "        for fname in filenames:\n",
    "            if fname[0] == '.' : # .으로 시작하는 파일명을 제외함 (ex: ._00~~~ 파일 제외)\n",
    "                continue\n",
    "            filename = os.path.join(dir_name, fname) # 폴더명, 파일명 \n",
    "            if os.path.isdir(filename): # 디렉토리 여부 확인\n",
    "                search(filename, result) # 하부 파일 목록 추출 (incorrect, mask, normal)\n",
    "            else:\n",
    "                ext = os.path.splitext(filename)[-1] # splitext()[-1] : 확장자 찾기 \n",
    "                if ext:\n",
    "                    result.append(filename) # 확장자 추가\n",
    "                # if ext != '.jpg':\n",
    "                #     print(ext)\n",
    "    except PermissionError: \n",
    "        print(\"파일의 경로 설정을 확인해보세요.\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_path = []\n",
    "search(train_img_dir_path, all_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18900"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_path = sorted(all_path) # 파일을 sorting 해줍니다. \n",
    "\n",
    "len(all_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "isdir과 listdir로 검색한 파일은 총 18900 개입니다.\n",
    "\n",
    "2700 개의 image 폴더는 각각 7개의 파일(normal, incorrect, mask1~5)로 구성되어있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/opt/ml/input/data/train/images/000001_female_Asian_45/incorrect_mask.jpg',\n",
       " '/opt/ml/input/data/train/images/000001_female_Asian_45/mask1.jpg',\n",
       " '/opt/ml/input/data/train/images/000001_female_Asian_45/mask2.jpg',\n",
       " '/opt/ml/input/data/train/images/000001_female_Asian_45/mask3.jpg',\n",
       " '/opt/ml/input/data/train/images/000001_female_Asian_45/mask4.jpg',\n",
       " '/opt/ml/input/data/train/images/000001_female_Asian_45/mask5.jpg',\n",
       " '/opt/ml/input/data/train/images/000001_female_Asian_45/normal.jpg',\n",
       " '/opt/ml/input/data/train/images/000002_female_Asian_52/incorrect_mask.jpg',\n",
       " '/opt/ml/input/data/train/images/000002_female_Asian_52/mask1.jpg',\n",
       " '/opt/ml/input/data/train/images/000002_female_Asian_52/mask2.jpg']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_path[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.jpg', '.png', '.jpeg']\n"
     ]
    }
   ],
   "source": [
    "# 파일 확장자 확인\n",
    "exts = []\n",
    "for filepath in all_path:\n",
    "    ext = os.path.splitext(filepath)[-1]\n",
    "    if ext not in exts:\n",
    "        exts.append(ext)\n",
    "print(exts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파일명을 분석하여 조건에 따라 class label list(정답)을 생성합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- class 0 : wear, male, age < 30\n",
    "- class 1 : wear, male, 30 <= age < 60\n",
    "- class 2 : wear, male, 60 <= age\n",
    "- class 4 : wear, female, age < 30\n",
    "- class 5 : wear, female, 30 <= age < 60\n",
    "- class 6 : incorrect, female, 60 <= age\n",
    "- class 7 : incorrect, male, age < 30\n",
    "- class 8 : incorrect, male, 30 <= age < 60\n",
    "- class 9 : incorrect, male, 60 <= age\n",
    "- class 10 : incorrect, female, age < 30\n",
    "- class 11 : incorrect, female, 30 <= age < 60\n",
    "- class 12 : not wear, female, 60 <= age\n",
    "- class 13 : not wear, male, age < 30\n",
    "- class 14 : not wear, male, 30 <= age < 60\n",
    "- class 15 : not wear, male, 60 <= age\n",
    "- class 16 : not wear, female, age < 30\n",
    "- class 17 : not wear, female, 30 <= age < 60\n",
    "- class 18 : not wear, female, 60 <= age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labeling : 조건에 따라 label에 숫자를 더해줍니다.\n",
    "def labeling(name):\n",
    "    label = 0\n",
    "    info, mask_type = name.split('/')[-2:] # info = 'image 폴더명', mask_type = '파일명'\n",
    "    info = info.split('_') # info = [index, gender, race, age]\n",
    "    gender, age = info[1], int(info[3])\n",
    "    if 'incorrect' in mask_type:\n",
    "        label += 6\n",
    "    elif 'normal' in mask_type:\n",
    "        label += 12\n",
    "\n",
    "    if gender == 'female':\n",
    "        label += 3\n",
    "    \n",
    "    if 30 <= age < 60:\n",
    "        label += 1\n",
    "    elif age >= 60:\n",
    "        label += 2\n",
    "        \n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/opt/ml/input/data/train/images/000001_female_...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/opt/ml/input/data/train/images/000001_female_...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/opt/ml/input/data/train/images/000001_female_...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/opt/ml/input/data/train/images/000001_female_...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/opt/ml/input/data/train/images/000001_female_...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18895</th>\n",
       "      <td>/opt/ml/input/data/train/images/006959_male_As...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18896</th>\n",
       "      <td>/opt/ml/input/data/train/images/006959_male_As...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18897</th>\n",
       "      <td>/opt/ml/input/data/train/images/006959_male_As...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18898</th>\n",
       "      <td>/opt/ml/input/data/train/images/006959_male_As...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18899</th>\n",
       "      <td>/opt/ml/input/data/train/images/006959_male_As...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18900 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    path  label\n",
       "0      /opt/ml/input/data/train/images/000001_female_...     10\n",
       "1      /opt/ml/input/data/train/images/000001_female_...      4\n",
       "2      /opt/ml/input/data/train/images/000001_female_...      4\n",
       "3      /opt/ml/input/data/train/images/000001_female_...      4\n",
       "4      /opt/ml/input/data/train/images/000001_female_...      4\n",
       "...                                                  ...    ...\n",
       "18895  /opt/ml/input/data/train/images/006959_male_As...      0\n",
       "18896  /opt/ml/input/data/train/images/006959_male_As...      0\n",
       "18897  /opt/ml/input/data/train/images/006959_male_As...      0\n",
       "18898  /opt/ml/input/data/train/images/006959_male_As...      0\n",
       "18899  /opt/ml/input/data/train/images/006959_male_As...     12\n",
       "\n",
       "[18900 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_path_label = pd.DataFrame(all_path, columns = ['path'])\n",
    "\n",
    "train_path_label['label'] = train_path_label['path'].map(lambda x : labeling(x))\n",
    "train_path_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y에서는 데이터셋을 좀 더 쉽게 쉽게 다룰 수 있도록 torch.utils.data.Dataset과 torch.utils.data.DataLoader를 제공합니다. \n",
    "\n",
    "이 torch.utils.data.Dataset을 상속받아 직접 Custom DataSet을 만들어서 사용하는 경우도 많습니다. \n",
    "\n",
    "(reference : https://wikidocs.net/57165)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset을 상속받아 만든 CustomDataset 입니다. \n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, img_paths_label, transform):\n",
    "        self.X = img_paths_label['path']\n",
    "        self.y = img_paths_label['label']\n",
    "        self.transform = transform\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.X.iloc[index])\n",
    "        label = self.y.iloc[index]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image) # torchvision 사용시\n",
    "            #image = self.transform(image=np.array(image))[\"image\"] # albumentations 사용시\n",
    "\n",
    "        return image, torch.tensor(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchvision에서 제공해주는 다양한 함수를 이용하여 입력 영상을 전처리합니다.\n",
    "# [512, 384] 크기로 영상을 resize하고 데이터를 Tensor로 만들어줍니다.\n",
    "# 이후 정규화를 해줍니다. ([0,255]->[0, 1]로 표현합니다.  )\n",
    "\n",
    "train_transform  = transforms.Compose([\n",
    "    #CenterCrop((320, 256)),\n",
    "    Resize((512, 384), Image.BILINEAR),\n",
    "    RandomHorizontalFlip(p=0.3), # randomly H_flip images\n",
    "    #ColorJitter(brightness=0.1), # randomly change color space\n",
    "    #RandomPerspective(distortion_scale=0.3, p=0.2),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=(0.5, 0.5, 0.5), std=(0.2, 0.2, 0.2)),\n",
    "    #AddGaussianNoise(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_transform = A.Compose(\n",
    "#     [\n",
    "#         A.CenterCrop(height=320, width=256, p=1.0),\n",
    "#         A.Resize(512, 384),\n",
    "#         A.HorizontalFlip(p=0.3),\n",
    "#         A.CoarseDropout(p=0.3),\n",
    "#         A.RandomBrightnessContrast(brightness_limit=(-0.3, 0.3), contrast_limit=(-0.3, 0.3), p=0.3),    \n",
    "#         A.OneOf(\n",
    "#             [\n",
    "#                 A.GridDistortion(p=0.2),\n",
    "#                 A.OpticalDistortion(p=0.2),\n",
    "#             ],\n",
    "#             p=1,\n",
    "#         ),\n",
    "#         A.OneOf(\n",
    "#             [\n",
    "#                 A.ChannelShuffle(p=0.2),  \n",
    "#                 A.GaussNoise(var_limit=(1000, 1600), p=0.2),\n",
    "#                 A.GaussianBlur(p=0.2),\n",
    "#                 A.GlassBlur(p=0.2),\n",
    "#                 A.MedianBlur(p=0.2),\n",
    "#                 A.MotionBlur(p=0.2),  \n",
    "#                 A.RGBShift(p=0.2),\n",
    "#             ],\n",
    "#             p=1,\n",
    "#         ),       \n",
    "#         A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.2, 0.2, 0.2]),\n",
    "#         ToTensorV2(p=1.0),\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_transform = transforms.Compose([\n",
    "    Resize((512, 384), Image.BILINEAR),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=(0.5, 0.5, 0.5), std=(0.2, 0.2, 0.2)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid_transform = A.Compose(\n",
    "#     [\n",
    "#         A.Resize(512, 384),\n",
    "#         A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.2, 0.2, 0.2]),\n",
    "#         ToTensorV2(p=1.0),\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train data와 validation data를 구분합니다.\n",
    "\n",
    "label의 비율을 유지하면서 분할합니다.\n",
    "\n",
    "train data : 15120 (80%)\n",
    "\n",
    "validation data : 3780 (20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of train dataset : 15120\n",
      "The number of valid dataset : 3780\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, valid = train_test_split(train_path_label, test_size = 0.2, \n",
    "                                shuffle = True, stratify = train_path_label['label'],\n",
    "                                random_state = 34 )\n",
    "\n",
    "print(\"The number of train dataset :\", len(train))\n",
    "print(\"The number of valid dataset :\", len(valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size 는 64로 설정하고, shuffle을 하여 DataLoader를 정의했습니다.\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_dataset = CustomDataset(train, train_transform)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, \n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            shuffle=True\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = CustomDataset(valid, valid_transform)\n",
    "\n",
    "valid_dataloader = DataLoader(valid_dataset,\n",
    "                            batch_size = BATCH_SIZE,\n",
    "                            shuffle = True\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3, 512, 384])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataloader는 [batchsize, channel, height, wide] 를 출력합니다. \n",
    "next(iter(train_dataloader))[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Model 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip instasll --upgrade jupyter ipywidgets\n",
    "model = torchvision.models.resnet34(pretrained = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "OUTPUT_CLASS_NUM = 18\n",
    "model.fc = torch.nn.Linear(in_features = 512, out_features = OUTPUT_CLASS_NUM, bias = True) \n",
    "\n",
    "# xavier uniform : 이전 노드와 다음 노드의 개수에 의존하는 초기화 방법으로 uniform/normal initialization의 두 가지 유형이 있습니다.\n",
    "torch.nn.init.xavier_uniform_(model.fc.weight)\n",
    "stdv = 1. / math.sqrt(model.fc.weight.size(1))\n",
    "model.fc.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "model.fc.weight.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights in weighted loss\n",
    "\n",
    "[ref] https://discuss.pytorch.org/t/weights-in-weighted-loss-nn-crossentropyloss/69514/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss weight : tensor([0.8534, 0.8921, 0.9775, 0.8077, 0.7836, 0.9714, 0.9707, 0.9784, 0.9955,\n",
      "        0.9615, 0.9567, 0.9943, 0.9707, 0.9784, 0.9955, 0.9615, 0.9567, 0.9943],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Cross Entropy Loss Weight를 적용합니다.\n",
    "# Class에 해당하는 데이터 비율에 반비례하도록 weight 설정합니다.\n",
    "\n",
    "loss_weight = torch.zeros(OUTPUT_CLASS_NUM, dtype = torch.float32).to(device)\n",
    "\n",
    "for i in range(OUTPUT_CLASS_NUM):\n",
    "    loss_weight[i] = len(train_path_label[train_path_label['label']==i])\n",
    "\n",
    "loss_weight = loss_weight / loss_weight.sum()\n",
    "loss_weight = (1-loss_weight)\n",
    "print('loss weight :', loss_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "\n",
    "LEARNING_RATE = 0.0001 # 학습에서 사용하는 optimizer의 학습률 옵션 설정\n",
    "NUM_EPOCH = 20 # 학습에서 데이터셋을 얼마나 많이 학습할 지 반복 횟수를 결정하는 옵션\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(weight = loss_weight) # 분류 학습에서 많이 사용하는 Cross Entropy loss를 objective function 적용\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE) # weight updqte를 위한 optimizer를 Adam으로 설정\n",
    "\n",
    "dataloaders = {\n",
    "    \"train\" : train_dataloader,\n",
    "    \"test\" : valid_dataloader\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ace1e1e49985458b8df6b21061a86bf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-0의 train-데이터 셋에서 평균 Loss :  0.383, 평균 Accuracy :  0.886\n",
      "현재 epoch-0의 test-데이터 셋에서 평균 Loss :  0.138, 평균 Accuracy :  0.957\n",
      "현재 epoch-1의 train-데이터 셋에서 평균 Loss :  0.047, 평균 Accuracy :  0.987\n",
      "현재 epoch-1의 test-데이터 셋에서 평균 Loss :  0.080, 평균 Accuracy :  0.974\n",
      "현재 epoch-2의 train-데이터 셋에서 평균 Loss :  0.019, 평균 Accuracy :  0.996\n",
      "현재 epoch-2의 test-데이터 셋에서 평균 Loss :  0.069, 평균 Accuracy :  0.978\n",
      "현재 epoch-3의 train-데이터 셋에서 평균 Loss :  0.023, 평균 Accuracy :  0.994\n",
      "현재 epoch-3의 test-데이터 셋에서 평균 Loss :  0.081, 평균 Accuracy :  0.979\n",
      "현재 epoch-4의 train-데이터 셋에서 평균 Loss :  0.031, 평균 Accuracy :  0.991\n",
      "현재 epoch-4의 test-데이터 셋에서 평균 Loss :  0.046, 평균 Accuracy :  0.987\n",
      "현재 epoch-5의 train-데이터 셋에서 평균 Loss :  0.010, 평균 Accuracy :  0.998\n",
      "현재 epoch-5의 test-데이터 셋에서 평균 Loss :  0.033, 평균 Accuracy :  0.991\n",
      "현재 epoch-6의 train-데이터 셋에서 평균 Loss :  0.017, 평균 Accuracy :  0.995\n",
      "현재 epoch-6의 test-데이터 셋에서 평균 Loss :  0.046, 평균 Accuracy :  0.986\n",
      "현재 epoch-7의 train-데이터 셋에서 평균 Loss :  0.017, 평균 Accuracy :  0.995\n",
      "현재 epoch-7의 test-데이터 셋에서 평균 Loss :  0.150, 평균 Accuracy :  0.957\n",
      "현재 epoch-8의 train-데이터 셋에서 평균 Loss :  0.019, 평균 Accuracy :  0.995\n",
      "현재 epoch-8의 test-데이터 셋에서 평균 Loss :  0.071, 평균 Accuracy :  0.978\n",
      "현재 epoch-9의 train-데이터 셋에서 평균 Loss :  0.005, 평균 Accuracy :  0.999\n",
      "현재 epoch-9의 test-데이터 셋에서 평균 Loss :  0.016, 평균 Accuracy :  0.995\n",
      "현재 epoch-10의 train-데이터 셋에서 평균 Loss :  0.001, 평균 Accuracy :  1.000\n",
      "현재 epoch-10의 test-데이터 셋에서 평균 Loss :  0.019, 평균 Accuracy :  0.995\n",
      "현재 epoch-11의 train-데이터 셋에서 평균 Loss :  0.000, 평균 Accuracy :  1.000\n",
      "현재 epoch-11의 test-데이터 셋에서 평균 Loss :  0.012, 평균 Accuracy :  0.998\n",
      "현재 epoch-12의 train-데이터 셋에서 평균 Loss :  0.000, 평균 Accuracy :  1.000\n",
      "현재 epoch-12의 test-데이터 셋에서 평균 Loss :  0.013, 평균 Accuracy :  0.997\n",
      "현재 epoch-13의 train-데이터 셋에서 평균 Loss :  0.000, 평균 Accuracy :  1.000\n",
      "현재 epoch-13의 test-데이터 셋에서 평균 Loss :  0.011, 평균 Accuracy :  0.998\n",
      "현재 epoch-14의 train-데이터 셋에서 평균 Loss :  0.000, 평균 Accuracy :  1.000\n",
      "현재 epoch-14의 test-데이터 셋에서 평균 Loss :  0.010, 평균 Accuracy :  0.998\n",
      "현재 epoch-15의 train-데이터 셋에서 평균 Loss :  0.000, 평균 Accuracy :  1.000\n",
      "현재 epoch-15의 test-데이터 셋에서 평균 Loss :  0.010, 평균 Accuracy :  0.998\n",
      "현재 epoch-16의 train-데이터 셋에서 평균 Loss :  0.000, 평균 Accuracy :  1.000\n",
      "현재 epoch-16의 test-데이터 셋에서 평균 Loss :  0.011, 평균 Accuracy :  0.997\n",
      "현재 epoch-17의 train-데이터 셋에서 평균 Loss :  0.000, 평균 Accuracy :  1.000\n",
      "현재 epoch-17의 test-데이터 셋에서 평균 Loss :  0.012, 평균 Accuracy :  0.998\n",
      "현재 epoch-18의 train-데이터 셋에서 평균 Loss :  0.000, 평균 Accuracy :  1.000\n",
      "현재 epoch-18의 test-데이터 셋에서 평균 Loss :  0.013, 평균 Accuracy :  0.996\n",
      "현재 epoch-19의 train-데이터 셋에서 평균 Loss :  0.000, 평균 Accuracy :  1.000\n",
      "현재 epoch-19의 test-데이터 셋에서 평균 Loss :  0.011, 평균 Accuracy :  0.996\n",
      "\n",
      "학습 종료!\n",
      "최고 accuracy : 0.9976190328598022, 최고 낮은 loss: 0.01041597760817259\n"
     ]
    }
   ],
   "source": [
    "best_test_accuracy = 0.\n",
    "best_test_loss = 9999.\n",
    "\n",
    "for epoch in tqdm(range(NUM_EPOCH)):\n",
    "    # train_dataset = CustomDataset(train, train_transform)\n",
    "\n",
    "    # train_dataloader = DataLoader(train_dataset, \n",
    "    #                             batch_size=BATCH_SIZE,\n",
    "    #                             shuffle=True\n",
    "    #                             )\n",
    "    for phase in [\"train\", \"test\"]:\n",
    "        running_loss = 0.\n",
    "        running_acc = 0.\n",
    "        if phase == \"train\":\n",
    "            model.train() # 네트워크 모델을 train 모드로 두어 gradient를 계산하고 여러 sub module (배치 정규화, 드롭아웃 등 )이 train mode로 작동할 수 있도록 함\n",
    "        elif phase == \"test\":\n",
    "            model.eval() # 네트워크 모델을 eval 모드로 부여하여 여러 sub module들이 eval mode로 작동할 수 있도록 설정 \n",
    "\n",
    "        for ind, (images, labels) in enumerate(dataloaders[phase]):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad() # parameter gradient를 업데이트 전 초기화 함\n",
    "\n",
    "            with torch.set_grad_enabled(phase == \"train\"): # train 모드일 시에는 gradient 를 계산하고, \n",
    "                                                           # 아닌 경우에는 gradient를 계산하지 않아 연산량이 최소화\n",
    "                logits = model(images)\n",
    "                _, preds = torch.max(logits, 1) # 모델에서 linear 값으로 나오는 예측 값을 최대 output index를 찾아 label([2])로 변경함\n",
    "                loss = loss_fn(logits, labels)\n",
    "\n",
    "                if phase == \"train\":\n",
    "                    loss.backward() # 모델의 예측 값과 실제 값의 CrossEntropy 차이를 통해 gradient 계산\n",
    "                    optimizer.step() # 계산된 gradient 를 가지고 모델 업데이트\n",
    "\n",
    "            running_loss += loss.item() * images.size(0) # 한 Batch에서의 loss 값 저장\n",
    "            running_acc += torch.sum(preds == labels.data) # 한 Batch에서의 Accuracy 값 저장\n",
    "            \n",
    "        # 한 EPOCH이 모두 종료되었을 때\n",
    "        epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "        epoch_acc = running_acc / len(dataloaders[phase].dataset)\n",
    "\n",
    "        print(f\"현재 epoch-{epoch}의 {phase}-데이터 셋에서 평균 Loss : {epoch_loss: .3f}, 평균 Accuracy : {epoch_acc: .3f}\")\n",
    "        if phase == \"test\" and best_test_accuracy < epoch_acc: # phase가 test일 때, best accuracy 계산\n",
    "            best_test_accuracy = epoch_acc\n",
    "        if phase == \"test\" and best_test_loss > epoch_loss : # phase가 test일 때 test loss 계산\n",
    "            best_test_loss = epoch_loss\n",
    "print(\"학습 종료!\")\n",
    "print(f\"최고 accuracy : {best_test_accuracy}, 최고 낮은 loss: {best_test_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_eval(model, data_iter, device):\n",
    "    with torch.no_grad():\n",
    "        n_total, n_correct = 0, 0\n",
    "        model.eval()\n",
    "        for batch_in, batch_out in data_iter :\n",
    "            y_trgt = batch_out.to(device)\n",
    "            model_pred = model.forward(batch_in.to(device))\n",
    "            _, y_pred = torch.max(model_pred, 1) # 행으로 비교\n",
    "            n_correct += (y_pred == y_trgt).sum().item()\n",
    "            n_total += batch_in.size(0)\n",
    "        val_accr = (n_correct / n_total)\n",
    "    return val_accr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9962962962962963"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func_eval(model, valid_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_eval(raw_data, dataloader, model, device):\n",
    "    result = []\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for i, (X,y) in enumerate(dataloader):\n",
    "            model_pred = model.forward(X.to(device))\n",
    "            _, y_pred = torch.max(model_pred,1)\n",
    "\n",
    "            result.append([valid.iloc[i]['path'], y_pred.cpu().numpy()[0], y.cpu().numpy()[0]])\n",
    "    result = pd.DataFrame(result, columns=['path', 'pred', 'target'])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>pred</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/opt/ml/input/data/train/images/003049_female_...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/opt/ml/input/data/train/images/001128_female_...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/opt/ml/input/data/train/images/003653_male_As...</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/opt/ml/input/data/train/images/003121_female_...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/opt/ml/input/data/train/images/005251_male_As...</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3775</th>\n",
       "      <td>/opt/ml/input/data/train/images/001809_male_As...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3776</th>\n",
       "      <td>/opt/ml/input/data/train/images/005415_female_...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3777</th>\n",
       "      <td>/opt/ml/input/data/train/images/003794_female_...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3778</th>\n",
       "      <td>/opt/ml/input/data/train/images/001024-1_femal...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3779</th>\n",
       "      <td>/opt/ml/input/data/train/images/003926_male_As...</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3780 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   path  pred  target\n",
       "0     /opt/ml/input/data/train/images/003049_female_...     3       3\n",
       "1     /opt/ml/input/data/train/images/001128_female_...     3       3\n",
       "2     /opt/ml/input/data/train/images/003653_male_As...     7       7\n",
       "3     /opt/ml/input/data/train/images/003121_female_...     3       3\n",
       "4     /opt/ml/input/data/train/images/005251_male_As...    12      12\n",
       "...                                                 ...   ...     ...\n",
       "3775  /opt/ml/input/data/train/images/001809_male_As...     0       0\n",
       "3776  /opt/ml/input/data/train/images/005415_female_...    10      10\n",
       "3777  /opt/ml/input/data/train/images/003794_female_...     4       4\n",
       "3778  /opt/ml/input/data/train/images/001024-1_femal...     3       3\n",
       "3779  /opt/ml/input/data/train/images/003926_male_As...     7       7\n",
       "\n",
       "[3780 rows x 3 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_testing_dataloader = DataLoader(valid_dataset, shuffle = False)\n",
    "\n",
    "check_eval_df = check_eval(valid, valid_testing_dataloader, model, device)\n",
    "check_eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>pred</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/opt/ml/input/data/train/images/001335_male_As...</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/opt/ml/input/data/train/images/000225_female_...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/opt/ml/input/data/train/images/003032_female_...</td>\n",
       "      <td>16</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/opt/ml/input/data/train/images/003440_male_As...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/opt/ml/input/data/train/images/006269_female_...</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>/opt/ml/input/data/train/images/006520_female_...</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>/opt/ml/input/data/train/images/004079_male_As...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>/opt/ml/input/data/train/images/001011_male_As...</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>/opt/ml/input/data/train/images/005497_male_As...</td>\n",
       "      <td>16</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>/opt/ml/input/data/train/images/001637_female_...</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>/opt/ml/input/data/train/images/005446_female_...</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>/opt/ml/input/data/train/images/000225_female_...</td>\n",
       "      <td>16</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>/opt/ml/input/data/train/images/000661_male_As...</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>/opt/ml/input/data/train/images/001335_male_As...</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 path  pred  target\n",
       "0   /opt/ml/input/data/train/images/001335_male_As...    12      13\n",
       "1   /opt/ml/input/data/train/images/000225_female_...     4       3\n",
       "2   /opt/ml/input/data/train/images/003032_female_...    16      15\n",
       "3   /opt/ml/input/data/train/images/003440_male_As...     1       2\n",
       "4   /opt/ml/input/data/train/images/006269_female_...     3       9\n",
       "5   /opt/ml/input/data/train/images/006520_female_...    15       9\n",
       "6   /opt/ml/input/data/train/images/004079_male_As...     5       1\n",
       "7   /opt/ml/input/data/train/images/001011_male_As...    12      13\n",
       "8   /opt/ml/input/data/train/images/005497_male_As...    16      13\n",
       "9   /opt/ml/input/data/train/images/001637_female_...    15      16\n",
       "10  /opt/ml/input/data/train/images/005446_female_...     4       5\n",
       "11  /opt/ml/input/data/train/images/000225_female_...    16      15\n",
       "12  /opt/ml/input/data/train/images/000661_male_As...    10       7\n",
       "13  /opt/ml/input/data/train/images/001335_male_As...     6       7"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrong_df = check_eval_df[check_eval_df['pred'] != check_eval_df['target']]\n",
    "wrong_df = wrong_df.reset_index(drop = True)\n",
    "wrong_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def draw_(df):\n",
    "    plt.figure(figsize = (15, 30))\n",
    "    row = 7\n",
    "    for i in range(df.shape[0]):\n",
    "        plt.subplot(row + 1, df.shape[0] // row, i+1)\n",
    "        plt.imshow(Image.open(df['path'][i]))\n",
    "        plt.title(f\"target:{df['target'][i]}, pred:{df['pred'][i]}\", color = 'r', size = 15)\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.Submit submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, img_paths, transform):\n",
    "        self.img_paths = img_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.img_paths[index])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta 데이터와 이미지 경로를 불러옵니다.\n",
    "submission = pd.read_csv(os.path.join(test_dir, 'info.csv'))\n",
    "image_dir = os.path.join(test_dir, 'images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Dataset 클래스 객체를 생성하고 DataLoader를 만듭니다.\n",
    "image_paths = [os.path.join(image_dir, img_id) for img_id in submission.ImageID]\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    Resize((512, 384), Image.BILINEAR),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=(0.5, 0.5, 0.5), std=(0.2, 0.2, 0.2)),\n",
    "])\n",
    "\n",
    "test_dataset = TestDataset(image_paths, transform)\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset,\n",
    "                            shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델이 테스트 데이터셋을 예측하고 결과를 저장합니다.\n",
    "\n",
    "def test(test_dataloader, model, device):\n",
    "    all_predictions = []\n",
    "    model.eval()\n",
    "    \n",
    "    for images in test_dataloader:\n",
    "        with torch.no_grad():\n",
    "            images = images.to(device)\n",
    "            pred = model(images)\n",
    "            pred = pred.argmax(dim=-1)\n",
    "            all_predictions.extend(pred.cpu().numpy())\n",
    "    submission['ans'] = all_predictions\n",
    "\n",
    "    return submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test inference is done!\n"
     ]
    }
   ],
   "source": [
    "# 제출할 파일을 저장합니다.\n",
    "\n",
    "submission = test(test_dataloader, model, device)\n",
    "\n",
    "submission.to_csv(os.path.join(test_dir, 'submission.csv'), index=False)\n",
    "print('test inference is done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
