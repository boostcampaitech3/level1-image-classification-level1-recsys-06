{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d732bdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1bb4726e",
   "metadata": {},
   "source": [
    "## Image Classification base code\n",
    "<br>\n",
    "이 파일은 boostcamp AI Tech에서 제공하는 <font color = \"yellow\"> **sample_submission.ipynb **</font>과 이민준 캠퍼님의 <font color = \"yellow\"> **resnet18 base code**</font>를 참고하여 작성했습니다. (test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "about-heavy",
   "metadata": {},
   "source": [
    "## 0. Libarary 불러오기 및 경로설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cubic-scoop",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms, utils\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "built-elevation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터셋 폴더 경로를 지정해주세요.\n",
    "test_dir = '/opt/ml/input/data/eval'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36edad3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 데이터셋 폴더 경로 지정\n",
    "train_dir = '/opt/ml/input/data/train'\n",
    "train_img_dir_path = os.path.join(train_dir, 'images')\n",
    "#train_img_dir_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "domestic-channels",
   "metadata": {},
   "source": [
    "## 1. Dataset 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cac1112c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir_name 디렉토리 하부에 있는 모든 파일을 검색합니다. \n",
    "def search(dir_name, result):\n",
    "    try:\n",
    "        filenames = os.listdir(dir_name)\n",
    "        for fname in filenames:\n",
    "            if fname[0] == '.' : # .으로 시작하는 파일명을 제외함 (ex: ._00~~~ 파일 제외)\n",
    "                continue\n",
    "            filename = os.path.join(dir_name, fname) # 폴더명, 파일명 \n",
    "            if os.path.isdir(filename): # 디렉토리 여부 확인\n",
    "                search(filename, result) # 하부 파일 목록 추출 (incorrect, mask, normal)\n",
    "            else:\n",
    "                ext = os.path.splitext(filename)[-1] # splitext()[-1] : 확장자 찾기 \n",
    "                if ext:\n",
    "                    result.append(filename) # 확장자 추가\n",
    "                # if ext != '.jpg':\n",
    "                #     print(ext)\n",
    "    except PermissionError: \n",
    "        print(\"파일의 경로 설정을 확인해보세요.\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7262e3df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18900"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_path = []\n",
    "search(train_img_dir_path, all_path)\n",
    "all_path = sorted(all_path) # 파일을 sorting 해줍니다. (왜할까???)\n",
    "\n",
    "len(all_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6812eabc",
   "metadata": {},
   "source": [
    "`isdir`과 `listdir`로 검색한 파일은 총 18900 개입니다. <br>\n",
    "2700 개의 image 폴더는 각각 7개의 파일(normal, incorrect, mask1~5)로 구성되어있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a60d03d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/opt/ml/input/data/train/images/000001_female_Asian_45/incorrect_mask.jpg',\n",
       " '/opt/ml/input/data/train/images/000001_female_Asian_45/mask1.jpg',\n",
       " '/opt/ml/input/data/train/images/000001_female_Asian_45/mask2.jpg',\n",
       " '/opt/ml/input/data/train/images/000001_female_Asian_45/mask3.jpg',\n",
       " '/opt/ml/input/data/train/images/000001_female_Asian_45/mask4.jpg',\n",
       " '/opt/ml/input/data/train/images/000001_female_Asian_45/mask5.jpg',\n",
       " '/opt/ml/input/data/train/images/000001_female_Asian_45/normal.jpg',\n",
       " '/opt/ml/input/data/train/images/000002_female_Asian_52/incorrect_mask.jpg',\n",
       " '/opt/ml/input/data/train/images/000002_female_Asian_52/mask1.jpg',\n",
       " '/opt/ml/input/data/train/images/000002_female_Asian_52/mask2.jpg']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_path[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ce46b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.jpg', '.png', '.jpeg']\n"
     ]
    }
   ],
   "source": [
    "# 파일 확장자 확인\n",
    "exts = []\n",
    "for filepath in all_path:\n",
    "    ext = os.path.splitext(filepath)[-1]\n",
    "    if ext not in exts:\n",
    "        exts.append(ext)\n",
    "print(exts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08082e34",
   "metadata": {},
   "source": [
    "image 파일의 확장자는 `['.jpg', '.jpeg', '.png']`의 3 종류가 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1a2685",
   "metadata": {},
   "source": [
    "#### 파일명을 분석하여 조건에 따라 class label list(정답)을 생성합니다. \n",
    "+ class 0 : mask, male, age <30\n",
    "+ class 1 : mask, male, 30 <= age < 60\n",
    "+ class 2 : mask, male, 60 <= age\n",
    "+ class 4 : mask, female, age <30\n",
    "+ class 5 : mask, female, 30 <= age < 60\n",
    "+ class 6 : mask, female, 60 <= age\n",
    "+ class 7 : incorrect, male, age <30\n",
    "+ class 8 : incorrect, male, 30 <= age < 60\n",
    "+ class 9 : incorrect, male, 60 <= age\n",
    "+ class 10 : incorrect, female, age <30\n",
    "+ class 11 : incorrect, female, 30 <= age < 60\n",
    "+ class 12 : incorrect, female, 60 <= age\n",
    "+ class 13 : normal, male, age <30\n",
    "+ class 14 : normal, male, 30 <= age < 60\n",
    "+ class 15 : normal, male, 60 <= age\n",
    "+ class 16 : normal, female, age <30\n",
    "+ class 17 : normal, female, 30 <= age < 60\n",
    "+ class 18 : normal, female, 60 <= age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "427f4c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labeling : 조건에 따라 label에 숫자를 더해줍니다.\n",
    "def labeling(name):\n",
    "    label = 0\n",
    "    info, mask_type = name.split('/')[-2:] # info = 'image 폴더명', mask_type = '파일명'\n",
    "    info = info.split('_') # info = [index, gender, race, age]\n",
    "    gender, age = info[1], int(info[3])\n",
    "    if 'incorrect' in mask_type:\n",
    "        label += 6\n",
    "    elif 'normal' in mask_type:\n",
    "        label += 12\n",
    "\n",
    "    if gender == 'female':\n",
    "        label += 3\n",
    "    \n",
    "    if 30 <= age < 60:\n",
    "        label += 1\n",
    "    elif age >= 60:\n",
    "        label += 2\n",
    "        \n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d33d020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/opt/ml/input/data/train/images/000001_female_...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/opt/ml/input/data/train/images/000001_female_...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/opt/ml/input/data/train/images/000001_female_...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/opt/ml/input/data/train/images/000001_female_...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/opt/ml/input/data/train/images/000001_female_...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18895</th>\n",
       "      <td>/opt/ml/input/data/train/images/006959_male_As...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18896</th>\n",
       "      <td>/opt/ml/input/data/train/images/006959_male_As...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18897</th>\n",
       "      <td>/opt/ml/input/data/train/images/006959_male_As...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18898</th>\n",
       "      <td>/opt/ml/input/data/train/images/006959_male_As...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18899</th>\n",
       "      <td>/opt/ml/input/data/train/images/006959_male_As...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18900 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    path  label\n",
       "0      /opt/ml/input/data/train/images/000001_female_...     10\n",
       "1      /opt/ml/input/data/train/images/000001_female_...      4\n",
       "2      /opt/ml/input/data/train/images/000001_female_...      4\n",
       "3      /opt/ml/input/data/train/images/000001_female_...      4\n",
       "4      /opt/ml/input/data/train/images/000001_female_...      4\n",
       "...                                                  ...    ...\n",
       "18895  /opt/ml/input/data/train/images/006959_male_As...      0\n",
       "18896  /opt/ml/input/data/train/images/006959_male_As...      0\n",
       "18897  /opt/ml/input/data/train/images/006959_male_As...      0\n",
       "18898  /opt/ml/input/data/train/images/006959_male_As...      0\n",
       "18899  /opt/ml/input/data/train/images/006959_male_As...     12\n",
       "\n",
       "[18900 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_path_label = pd.DataFrame(all_path, columns = ['path'])\n",
    "\n",
    "train_path_label['label'] = train_path_label['path'].map(lambda x : labeling(x))\n",
    "train_path_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f302280d",
   "metadata": {},
   "source": [
    "pytorch 에서는 데이터셋을 좀 더 쉽게 쉽게 다룰 수 있도록 `torch.utils.data.Dataset`과 `torch.utils.data.DataLoader`를 제공합니다. 이 torch.utils.data.Dataset을 상속받아 직접 Custom DataSet을 만들어서 사용하는 경우도 많습니다. \n",
    "(reference : https://wikidocs.net/57165)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6977d5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset을 상속받아 만든 CustomDataset 입니다. \n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, img_paths_label, transform):\n",
    "        self.X = img_paths_label['path']\n",
    "        self.y = img_paths_label['label']\n",
    "        self.transform = transform\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.X.iloc[index])\n",
    "        label = self.y.iloc[index]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, torch.tensor(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9046c231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchvision에서 제공해주는 다양한 함수를 이용하여 입력 영상을 전처리합니다.\n",
    "# [512, 384] 크기로 영상을 resize하고 데이터를 Tensor로 만들어줍니다.\n",
    "# 이후 정규화를 해줍니다. ([0,255]->[0, 1]로 표현합니다.  )\n",
    "transform  = transforms.Compose([\n",
    "    Resize((512, 384), Image.BILINEAR),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=(0.5, 0.5, 0.5), std=(0.2, 0.2, 0.2)),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60813555",
   "metadata": {},
   "source": [
    "train data와 validation data를 구분합니다.<br>\n",
    "label의 비율을 유지하면서 분할합니다.\n",
    "+ train data : 15120 (80%)\n",
    "+ validation data : 3780 (20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dacaa7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, valid = train_test_split(train_path_label, test_size = 0.2, \n",
    "                                shuffle = True, stratify = train_path_label['label'],\n",
    "                                random_state = 34 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5d6166b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15120, 2), (3780, 2))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d153ec5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size 는 64로 설정하고, shuffle을 하여 DataLoader를 정의했습니다.\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_dataset = CustomDataset(train, transform)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, \n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            shuffle=True\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c515df0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = CustomDataset(valid, transform)\n",
    "\n",
    "valid_dataloader = DataLoader(valid_dataset,\n",
    "                            batch_size = BATCH_SIZE,\n",
    "                            shuffle = True\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b4b7a0bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3, 512, 384])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataloader는 [batchsize, channel, height, wide] 를 출력합니다. \n",
    "next(iter(train_dataloader))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "extensive-north",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TestDataset(Dataset):\n",
    "#     def __init__(self, img_paths, transform):\n",
    "#         self.img_paths = img_paths\n",
    "#         self.transform = transform\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         image = Image.open(self.img_paths[index])\n",
    "\n",
    "#         if self.transform:\n",
    "#             image = self.transform(image)\n",
    "#         return image\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3d4818",
   "metadata": {},
   "source": [
    "## 2. Model 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "674f9f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18 = torchvision.models.resnet18(pretrained = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46dd7018",
   "metadata": {},
   "source": [
    "## *** <font color = \"red\"> resnet18 과 torchvision, 관련 paper 공부하기. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "08bdb38c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "OUTPUT_CLASS_NUM = 18\n",
    "resnet18.fc = torch.nn.Linear(in_features = 512, out_features = OUTPUT_CLASS_NUM, bias = True) \n",
    "\n",
    "# xavier uniform : 이전 노드와 다음 노드의 개수에 의존하는 초기화 방법으로 uniform/normal initialization의 두 가지 유형이 있습니다.\n",
    "torch.nn.init.xavier_uniform_(resnet18.fc.weight)\n",
    "stdv = 1. / math.sqrt(resnet18.fc.weight.size(1))\n",
    "resnet18.fc.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "resnet18.fc.weight.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6b46e2f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a380092a",
   "metadata": {},
   "source": [
    "#### <font color = \"red\"> boostcamp 모델링 부분 실습 파일 다시 공부해보기!! </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "06ba7460",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18.to(device)\n",
    "\n",
    "LEARNING_RATE = 0.0001 # 학습에서 사용하는 optimizer의 학습률 옵션 설정\n",
    "NUM_EPOCH = 5# 학습에서 데이터셋을 얼마나 많이 학습할 지 반복 횟수를 결정하는 옵션\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss() # 분류 학습에서 많이 사용하는 Cross Entropy loss를 objective functionh으로 사용\n",
    "optimizer = torch.optim.Adam(resnet18.parameters(), lr=LEARNING_RATE) # weight updqte를 위한 optimizer를 Adam으로 설정\n",
    "\n",
    "dataloaders = {\n",
    "    \"train\" : train_dataloader,\n",
    "    \"test\" : valid_dataloader\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coral-shade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test inference is done!\n"
     ]
    }
   ],
   "source": [
    "# # meta 데이터와 이미지 경로를 불러옵니다.\n",
    "# submission = pd.read_csv(os.path.join(test_dir, 'info.csv'))\n",
    "# image_dir = os.path.join(test_dir, 'images')\n",
    "\n",
    "# # Test Dataset 클래스 객체를 생성하고 DataLoader를 만듭니다.\n",
    "# image_paths = [os.path.join(image_dir, img_id) for img_id in submission.ImageID]\n",
    "# transform = transforms.Compose([\n",
    "#     Resize((512, 384), Image.BILINEAR),\n",
    "#     ToTensor(),\n",
    "#     Normalize(mean=(0.5, 0.5, 0.5), std=(0.2, 0.2, 0.2)),\n",
    "# ])\n",
    "# dataset = TestDataset(image_paths, transform)\n",
    "\n",
    "# loader = DataLoader(\n",
    "#     dataset,\n",
    "#     shuffle=False\n",
    "# )\n",
    "\n",
    "# # 모델을 정의합니다. (학습한 모델이 있다면 torch.load로 모델을 불러주세요!)\n",
    "# device = torch.device('cuda')\n",
    "# model = MyModel(num_classes=18).to(device)\n",
    "# model.eval()\n",
    "\n",
    "# # 모델이 테스트 데이터셋을 예측하고 결과를 저장합니다.\n",
    "# all_predictions = []\n",
    "# for images in loader:\n",
    "#     with torch.no_grad():\n",
    "#         images = images.to(device)\n",
    "#         pred = model(images)\n",
    "#         pred = pred.argmax(dim=-1)\n",
    "#         all_predictions.extend(pred.cpu().numpy())\n",
    "# submission['ans'] = all_predictions\n",
    "\n",
    "# # 제출할 파일을 저장합니다.\n",
    "# submission.to_csv(os.path.join(test_dir, 'submission.csv'), index=False)\n",
    "# print('test inference is done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acknowledged-easter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MyModel(nn.Module):\n",
    "#     def __init__(self, num_classes: int = 1000):\n",
    "#         super(MyModel, self).__init__()\n",
    "#         self.features = nn.Sequential(\n",
    "#             nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "#             nn.BatchNorm2d(64),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#         )\n",
    "#         self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.Dropout(),\n",
    "#             nn.Linear(64, 32),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Linear(32, num_classes),\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "#         x = self.features(x)\n",
    "#         x = self.avgpool(x)\n",
    "#         x = torch.flatten(x, 1)\n",
    "#         x = self.classifier(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continued-feelings",
   "metadata": {},
   "source": [
    "## 3. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f45daf6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 epoch-0의 train-데이터 셋에서 평균 Loss :  0.028, 평균 Accuracy :  0.992\n",
      "현재 epoch-0의 test-데이터 셋에서 평균 Loss :  0.194, 평균 Accuracy :  0.941\n",
      "현재 epoch-1의 train-데이터 셋에서 평균 Loss :  0.013, 평균 Accuracy :  0.997\n",
      "현재 epoch-1의 test-데이터 셋에서 평균 Loss :  0.032, 평균 Accuracy :  0.990\n",
      "현재 epoch-2의 train-데이터 셋에서 평균 Loss :  0.007, 평균 Accuracy :  0.998\n",
      "현재 epoch-2의 test-데이터 셋에서 평균 Loss :  0.078, 평균 Accuracy :  0.975\n",
      "현재 epoch-3의 train-데이터 셋에서 평균 Loss :  0.016, 평균 Accuracy :  0.996\n",
      "현재 epoch-3의 test-데이터 셋에서 평균 Loss :  0.055, 평균 Accuracy :  0.984\n",
      "현재 epoch-4의 train-데이터 셋에서 평균 Loss :  0.019, 평균 Accuracy :  0.995\n",
      "현재 epoch-4의 test-데이터 셋에서 평균 Loss :  0.057, 평균 Accuracy :  0.982\n",
      "학습 종료!\n",
      "최고 accuracy : 0.9899470806121826, 최고 낮은 loss: 0.032355715939528726\n"
     ]
    }
   ],
   "source": [
    "best_test_accuracy = 0.\n",
    "best_test_loss = 9999.\n",
    "\n",
    "for epoch in range(NUM_EPOCH):\n",
    "    for phase in [\"train\", \"test\"]:\n",
    "        running_loss = 0.\n",
    "        running_acc = 0.\n",
    "        if phase == \"train\":\n",
    "            resnet18.train() # 네트워크 모델을 train 모드로 두어 gradient를 계산하고 여러 sub module (배치 정규화, 드롭아웃 등 )이 train mode로 작동할 수 있도록 함\n",
    "        elif phase == \"test\":\n",
    "            resnet18.eval() # 네트워크 모델을 eval 모드로 부여하여 여러 sub module들이 eval mode로 작동할 수 있도록 설정 \n",
    "\n",
    "        for ind, (images, labels) in enumerate(dataloaders[phase]):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad() # parameter gradient를 업데이트 전 초기화 함\n",
    "\n",
    "            with torch.set_grad_enabled(phase == \"train\"): # train 모드일 시에는 gradient 를 계산하고, \n",
    "                                                           # 아닌 경우에는 gradient를 계산하지 않아 연산량이 최소화\n",
    "                logits = resnet18(images)\n",
    "                _, preds = torch.max(logits, 1) # 모델에서 linear 값으로 나오는 예측 값을 최대 output index를 찾아 label([2])로 변경함\n",
    "                loss = loss_fn(logits, labels)\n",
    "\n",
    "                if phase == \"train\":\n",
    "                    loss.backward() # 모델의 예측 값과 실제 값의 CrossEntropy 차이를 통해 gradient 계산\n",
    "                    optimizer.step() # 계산된 gradient 를 가지고 모델 업데이트\n",
    "\n",
    "            running_loss += loss.item() * images.size(0) # 한 Batch에서의 loss 값 저장\n",
    "            running_acc += torch.sum(preds == labels.data) # 한 Batch에서의 Accuracy 값 저장\n",
    "            \n",
    "        # 한 EPOCH이 모두 종료되었을 때\n",
    "        epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "        epoch_acc = running_acc / len(dataloaders[phase].dataset)\n",
    "\n",
    "        print(f\"현재 epoch-{epoch}의 {phase}-데이터 셋에서 평균 Loss : {epoch_loss: .3f}, 평균 Accuracy : {epoch_acc: .3f}\")\n",
    "        if phase == \"test\" and best_test_accuracy < epoch_acc: # phase가 test일 때, best accuracy 계산\n",
    "            best_test_accuracy = epoch_acc\n",
    "        if phase == \"test\" and best_test_loss > epoch_loss : # phase가 test일 때 test loss 계산\n",
    "            best_test_loss = epoch_loss\n",
    "print(\"학습 종료!\")\n",
    "print(f\"최고 accuracy : {best_test_accuracy}, 최고 낮은 loss: {best_test_loss}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continued-feelings",
   "metadata": {},
   "source": [
    "## 4. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3a8b21a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_eval(model, data_iter, device):\n",
    "    with torch.no_grad():\n",
    "        n_total, n_correct = 0, 0\n",
    "        model.eval()\n",
    "        for batch_in, batch_out in data_iter :\n",
    "            y_trgt = batch_out.to(device)\n",
    "            model_pred = model.forward(batch_in.to(device))\n",
    "            _, y_pred = torch.max(model_pred, 1) # 행으로 비교\n",
    "            n_correct += (y_pred == y_trgt).sum().item()\n",
    "            n_total += batch_in.size(0)\n",
    "        val_accr = (n_correct / n_total)\n",
    "    return val_accr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "42bf92ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9822751322751323"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func_eval(resnet18, valid_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d67dc02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_eval(raw_data, dataloader, model, device):\n",
    "    result = []\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for i, (X,y) in enumerate(dataloader):\n",
    "            model_pred = model.forward(X.to(device))\n",
    "            _, y_pred = torch.max(model_pred,1)\n",
    "\n",
    "            result.append([valid.iloc[i]['path'], y_pred.cpu().numpy()[0], y.cpu().numpy()[0]])\n",
    "    result = pd.DataFrame(result, columns=['path', 'pred', 'target'])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "228407d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>pred</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/opt/ml/input/data/train/images/003403_male_As...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/opt/ml/input/data/train/images/000735_female_...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/opt/ml/input/data/train/images/006365_male_As...</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/opt/ml/input/data/train/images/003434_male_As...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/opt/ml/input/data/train/images/001603_male_As...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3775</th>\n",
       "      <td>/opt/ml/input/data/train/images/004321_female_...</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3776</th>\n",
       "      <td>/opt/ml/input/data/train/images/004088_female_...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3777</th>\n",
       "      <td>/opt/ml/input/data/train/images/001490_female_...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3778</th>\n",
       "      <td>/opt/ml/input/data/train/images/005026_male_As...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3779</th>\n",
       "      <td>/opt/ml/input/data/train/images/003550_male_As...</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3780 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   path  pred  target\n",
       "0     /opt/ml/input/data/train/images/003403_male_As...     1       1\n",
       "1     /opt/ml/input/data/train/images/000735_female_...     4       4\n",
       "2     /opt/ml/input/data/train/images/006365_male_As...     6       6\n",
       "3     /opt/ml/input/data/train/images/003434_male_As...     1       1\n",
       "4     /opt/ml/input/data/train/images/001603_male_As...     0       0\n",
       "...                                                 ...   ...     ...\n",
       "3775  /opt/ml/input/data/train/images/004321_female_...    16      16\n",
       "3776  /opt/ml/input/data/train/images/004088_female_...     5       5\n",
       "3777  /opt/ml/input/data/train/images/001490_female_...     4       4\n",
       "3778  /opt/ml/input/data/train/images/005026_male_As...     2       2\n",
       "3779  /opt/ml/input/data/train/images/003550_male_As...    13      13\n",
       "\n",
       "[3780 rows x 3 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_testing_dataloader = DataLoader(valid_dataset, shuffle = False)\n",
    "\n",
    "check_eval_df = check_eval(valid, valid_testing_dataloader, resnet18, device)\n",
    "check_eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "verbal-sample",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>pred</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/opt/ml/input/data/train/images/001578_male_As...</td>\n",
       "      <td>16</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/opt/ml/input/data/train/images/001528_female_...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/opt/ml/input/data/train/images/000225_female_...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/opt/ml/input/data/train/images/001652_female_...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/opt/ml/input/data/train/images/005555_male_As...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>/opt/ml/input/data/train/images/005227_male_As...</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>/opt/ml/input/data/train/images/004418_male_As...</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>/opt/ml/input/data/train/images/003889_female_...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>/opt/ml/input/data/train/images/001124_male_As...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>/opt/ml/input/data/train/images/003789_male_As...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 path  pred  target\n",
       "0   /opt/ml/input/data/train/images/001578_male_As...    16      13\n",
       "1   /opt/ml/input/data/train/images/001528_female_...     3       4\n",
       "2   /opt/ml/input/data/train/images/000225_female_...     4       3\n",
       "3   /opt/ml/input/data/train/images/001652_female_...     3       4\n",
       "4   /opt/ml/input/data/train/images/005555_male_As...     0       1\n",
       "..                                                ...   ...     ...\n",
       "62  /opt/ml/input/data/train/images/005227_male_As...    12       6\n",
       "63  /opt/ml/input/data/train/images/004418_male_As...     6      12\n",
       "64  /opt/ml/input/data/train/images/003889_female_...     3       4\n",
       "65  /opt/ml/input/data/train/images/001124_male_As...     0       1\n",
       "66  /opt/ml/input/data/train/images/003789_male_As...     4       1\n",
       "\n",
       "[67 rows x 3 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrong_df = check_eval_df[check_eval_df['pred'] != check_eval_df['target']]\n",
    "wrong_df = wrong_df.reset_index(drop = True)\n",
    "wrong_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0028b500",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_(df):\n",
    "    plt.figure(figsize = (15, 30))\n",
    "    row = 7\n",
    "    for i in range(df.shape[0]):\n",
    "        plt.subplot(row + 1, df.shape[0] // row, i+1)\n",
    "        plt.imshow(Image.open(df['path'][i]))\n",
    "        plt.title(f\"target:{df['target'][i]}, pred:{df['pred'][i]}\", color = 'r', size = 20)\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
